{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3069d1c2-d996-422e-9bb1-cc35dedef6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Importing Tools \n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from openpyxl.styles import NamedStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc08c57-eb66-4838-8238-ae0b006f0ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 Reduce risk of a timeout by increasing limit to 30 minutes\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"1800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21a9981-c2a1-46a2-9e9b-419c78eea302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3 Loading the master hierarchies table from the lake mart\n",
    "df_master_hierarchies = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection_Queries/master_hierarchies_table.csv\")\n",
    "#display(df_master_hierarchies.limit(10))\n",
    "#print(f\"Number of rows in master hierarchies: {df_master_hierarchies.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6334b52-eb66-4ef5-a95c-afea7c3e7ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4 loading ICB to Region table\n",
    "df_icb_region = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_ICB_Region_DisplayNames.csv\")  # Ensure proper Azure credentials are configured for ADLS access.\n",
    "#display(df_icb_region.limit(10))\n",
    "#print(f\"Number of rows in icb_region: {df_icb_region.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9b0856-5c30-47d8-8e98-a08bab8926f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5 loading list of merged providers\n",
    "df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "#display(df_merged_providers.limit(10))\n",
    "#print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08aeef7-8c39-402c-be16-83f587b7c999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6 creating new provider code from the provider mapping table\n",
    "provider_code_mapping = df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "#display(df_merged_providers.limit(10))\n",
    "#print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373bf4a4-013f-42f6-81eb-76f55d979cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7a importing MHS metric list and internal ID\n",
    "mhs_metric_list = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/MHS\")\n",
    "#display(mhs_metric_list.limit(10))\n",
    "#print(f\"Number of rows in mhs_metric_list: {mhs_metric_list.count()}\")\n",
    "\n",
    "#7b importing MHS allowable org codes\n",
    "mhs_allowable_orgs = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/MHS/Allowable_Org_Codes_Status.csv\")\n",
    "#display(mhs_allowable_orgs.limit(10))\n",
    "#print(f\"Number of rows in mhs_allowable_orgs: {mhs_allowable_orgs.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2a5552-3d8d-4900-8371-c80952cf4b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8 Loading the core monthly snapshot data\n",
    "from pyspark.sql import functions as F\n",
    "df_op_activity_snapshot = spark.read.option(\"header\", \"true\").option(\"recursiveFileLookup\", \"true\").parquet(\n",
    "    \"abfss://reporting@udalstdatacuratedprod.dfs.core.windows.net/restricted/patientlevel/MESH/OPA/OPA_Core_Monthly_Snapshot/Published/1\"\n",
    ")\n",
    "#display(df_op_activity_snapshot.limit(10))\n",
    "\n",
    "# Show number of rows in the raw data\n",
    "row_count = df_op_activity_snapshot.count()\n",
    "print(f\"Number of rows in raw data: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f82af93-37d4-4281-b468-6e017347849f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9 Creating the wide table & inserting new column for merged providers with new merger codes and mapping to ICB and Region codes\n",
    "from pyspark.sql.functions import when, col, lit, create_map, coalesce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define valid treatment function codes\n",
    "VALID_TREATMENT_CODES = [\n",
    "    '100', '101', '102', '104', '105', '106', '108', '110', '111', '115', '120', '130', '140',\n",
    "    '144', '145', '301', '302', '303', '307', '320', '330', '340', '361', '400', '410', '420',\n",
    "    '430', '501', '502', '560', '650'\n",
    "]\n",
    "\n",
    "# Adding in the Treatment_Function_Code_New column\n",
    "opa_with_tfc = df_op_activity_snapshot.withColumn(\n",
    "    \"Treatment_Function_Code_New\",\n",
    "    when(col(\"Treatment_Function_Code\").isin(VALID_TREATMENT_CODES), col(\"Treatment_Function_Code\")).otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Add Treatment_Function_Group column using VALID_TREATMENT_CODES groupings\n",
    "opa_with_groups = opa_with_tfc.withColumn(\n",
    "    \"Treatment_Function_Group\",\n",
    "    when(col(\"Treatment_Function_Code_New\").isin(\"100\", \"102\", \"104\", \"105\", \"106\"), \"GS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"140\", \"144\", \"145\"), \"OMFS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"110\", \"111\", \"115\"), \"T&O\")\n",
    "     .otherwise(col(\"Treatment_Function_Code_New\"))\n",
    ")\n",
    "\n",
    "# Filter dataset for relevant years, admin category, TFC, and attendance\n",
    "opa_filtered = opa_with_groups.filter(\n",
    "    (col(\"Der_Financial_Year\").isin(\"2023/24\", \"2024/25\", \"2025/26\")) &  # This can be updated manually\n",
    "    (col(\"Administrative_Category\") == \"01\") &\n",
    "    (col(\"Treatment_Function_Code\") != \"812\") &\n",
    "    (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\"))\n",
    ")\n",
    "\n",
    "# Aggregates the metrics by month, provider, and Treatment_Function_Group\n",
    "opa_agg = opa_filtered.groupBy(\n",
    "    \"Der_Activity_Month\",\n",
    "    \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\"\n",
    ").agg(\n",
    "    # All contacts\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_NoProc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_NoProc\"),\n",
    "    # Face-to-face\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"1\"), 1).otherwise(0)).alias(\"F2F_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"2\"), 1).otherwise(0)).alias(\"F2F_FU\"),\n",
    "    # Remote\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"3\"), 1).otherwise(0)).alias(\"Remote_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"4\"), 1).otherwise(0)).alias(\"Remote_FU\"),\n",
    "    # Did not attends (DNAs)\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_DNA\"),\n",
    "    # 2WW DNA\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW_DNA\"),\n",
    "    # All 2WW appointments\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW\")\n",
    ")\n",
    "\n",
    "# Add \"All\" TFC totals by month and provider\n",
    "METRIC_COLS = [c for c in opa_agg.columns if c not in [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\"]]\n",
    "\n",
    "opa_all_tfc = opa_agg.groupBy(\"Der_Activity_Month\", \"Der_Provider_Code\").agg(\n",
    "    *[F.sum(col(c)).alias(c) for c in METRIC_COLS]\n",
    ").withColumn(\"Treatment_Function_Group\", lit(\"All\"))\n",
    "\n",
    "opa_final = opa_agg.unionByName(opa_all_tfc)\n",
    "\n",
    "# Order by results\n",
    "opa_final_ordered = opa_final.orderBy(\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\")\n",
    "\n",
    "# Inserted mapping code to build mapping_expr from df_merged_providers\n",
    "provider_code_mapping_dict = {\n",
    "    row['Old_Provider_Code']: row['New_Provider_Code']\n",
    "    for row in df_merged_providers.select(\"Old_Provider_Code\", \"New_Provider_Code\").distinct().collect()\n",
    "}\n",
    "\n",
    "mapping_list = []\n",
    "for k, v in provider_code_mapping_dict.items():\n",
    "    mapping_list.append(lit(k))\n",
    "    mapping_list.append(lit(v))\n",
    "\n",
    "mapping_expr = create_map(mapping_list)\n",
    "\n",
    "# Add \"Adj Org Code\" column based on provider_code_mapping\n",
    "opa_final_ordered_with_adj = opa_final_ordered.withColumn(\n",
    "    \"Adj Org Code\",\n",
    "    coalesce(mapping_expr.getItem(col(\"Der_Provider_Code\")), col(\"Der_Provider_Code\"))\n",
    ")\n",
    "\n",
    "# Add \"ICB\" column by joining to df_master_hierarchies on Organisation_Code and returning STP_Code\n",
    "opa_final_ordered_with_icb = opa_final_ordered_with_adj.join(\n",
    "    df_master_hierarchies.select(\n",
    "        F.col(\"Organisation_Code\").alias(\"join_org_code\"),\n",
    "        F.col(\"STP_Code\").alias(\"ICB\")\n",
    "    ),\n",
    "    opa_final_ordered_with_adj[\"Adj Org Code\"] == F.col(\"join_org_code\"),\n",
    "    \"left\"\n",
    ").drop(\"join_org_code\")\n",
    "\n",
    "# Add \"Region\" column by joining to df_icb_region on ICB column and returning Region_Code\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb.join(\n",
    "    df_icb_region.select(\n",
    "        F.col(\"ICB_Code\").alias(\"join_icb\"),\n",
    "        F.col(\"Region_Code\")\n",
    "    ),\n",
    "    opa_final_ordered_with_icb[\"ICB\"] == F.col(\"join_icb\"),\n",
    "    \"left\"\n",
    ").drop(\"join_icb\")\n",
    "\n",
    "from pyspark.sql.functions import last_day, to_date, concat_ws\n",
    "\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb_region.withColumn(\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    last_day(\n",
    "        to_date(\n",
    "            concat_ws(\n",
    "                '-',\n",
    "                col(\"Der_Activity_Month\").substr(1, 4),\n",
    "                col(\"Der_Activity_Month\").substr(5, 2),\n",
    "                lit(\"01\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# display(opa_final_ordered_with_icb_region.limit(10))\n",
    "\n",
    "opa_final_ordered_with_icb_region_row_count = opa_final_ordered_with_icb_region.count()\n",
    "# Drop unwanted columns, aggregate metrics, and sort the final table\n",
    "id_cols = [\"Der_Activity_Month_Date\", \"Treatment_Function_Group\", \"Region_Code\", \"ICB\", \"Adj Org Code\"]\n",
    "\n",
    "# Determine metric columns (exclude identifiers and the three columns to drop)\n",
    "metric_cols = [\n",
    "    c for c in opa_final_ordered_with_icb_region.columns\n",
    "    if c not in id_cols + [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Code_New\"]\n",
    "]\n",
    "\n",
    "opa_final_processed = (\n",
    "    opa_final_ordered_with_icb_region\n",
    "    .groupBy(*[F.col(c) for c in id_cols])\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in metric_cols])\n",
    "    .orderBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"ICB\", \"Adj Org Code\", \"Treatment_Function_Group\")\n",
    ")\n",
    "\n",
    "# display(opa_final_processed.limit(10))\n",
    "# print(f\"Number of rows in opa_final_processed: {opa_final_processed.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1177ed-00ad-44f3-8eda-776b6093dd07",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760706170841}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#10 — Safe metric calculation (robust to missing columns)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = opa_final_ordered_with_icb_region\n",
    "\n",
    "def safe_add(df, new_col, expr_fn, required_cols):\n",
    "    if all(c in df.columns for c in required_cols):\n",
    "        return df.withColumn(new_col, expr_fn(df))\n",
    "    else:\n",
    "        return df.withColumn(new_col, F.lit(None))\n",
    "\n",
    "metrics = [\n",
    "    (\"All_DNA_Over_All_Total\", lambda d: F.when(\n",
    "        (F.col(\"All_Total\") + F.col(\"All_DNA\")) != 0,\n",
    "        (F.col(\"All_DNA\") / (F.col(\"All_Total\") + F.col(\"All_DNA\"))) * 100\n",
    "    ), [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"All_DNA_Over_All_Total_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_Total\") + F.col(\"All_DNA\")) != 0,\n",
    "        (F.col(\"All_DNA\") / (F.col(\"All_Total\") + F.col(\"All_DNA\"))) * 100\n",
    "    ), [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"All_First_DNA_Over_All_First\", lambda d: F.when(\n",
    "        (F.col(\"All_First\") + F.col(\"All_First_DNA\")) != 0,\n",
    "        (F.col(\"All_First_DNA\") / (F.col(\"All_First\") + F.col(\"All_First_DNA\"))) * 100\n",
    "    ), [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_First_DNA_Over_All_First_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_First\") + F.col(\"All_First_DNA\")) != 0,\n",
    "        (F.col(\"All_First_DNA\") / (F.col(\"All_First\") + F.col(\"All_First_DNA\"))) * 100\n",
    "    ), [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_DNA_Over_All_FU\", lambda d: F.when(\n",
    "        (F.col(\"All_FU\") + F.col(\"All_FU_DNA\")) != 0,\n",
    "        (F.col(\"All_FU_DNA\") / (F.col(\"All_FU\") + F.col(\"All_FU_DNA\"))) * 100\n",
    "    ), [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_FU_DNA_Over_All_FU_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_FU\") + F.col(\"All_FU_DNA\")) != 0,\n",
    "        (F.col(\"All_FU_DNA\") / (F.col(\"All_FU\") + F.col(\"All_FU_DNA\"))) * 100\n",
    "    ), [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_2WW_DNA_Over_All_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_2WW\") != 0) & (F.col(\"All_2WW\").isNotNull()),\n",
    "        (F.col(\"All_2WW_DNA\") / F.col(\"All_2WW\")) * 100\n",
    "    ), [\"All_2WW_DNA\", \"All_2WW\"]),\n",
    "    (\"All_FU_2WW_DNA_Over_All_FU_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_FU_2WW\") != 0) & (F.col(\"All_FU_2WW\").isNotNull()),\n",
    "        (F.col(\"All_FU_2WW_DNA\") / F.col(\"All_FU_2WW\")) * 100\n",
    "    ), [\"All_FU_2WW_DNA\", \"All_FU_2WW\"]),\n",
    "    (\"All_First_2WW_DNA_Over_All_First_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_First_2WW\") != 0) & (F.col(\"All_First_2WW\").isNotNull()),\n",
    "        (F.col(\"All_First_2WW_DNA\") / F.col(\"All_First_2WW\")) * 100\n",
    "    ), [\"All_First_2WW_DNA\", \"All_First_2WW\"]),\n",
    "    (\"All_FU_NoProc_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"All_FU_NoProc\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"All_FU_NoProc\", \"All_FU\"]),\n",
    "    (\"All_FU_Proc_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"All_FU_Proc\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"All_FU_Proc\", \"All_FU\"]),\n",
    "    (\"All_FU_To_All_First\", lambda d: F.when(\n",
    "        F.col(\"All_First\") != 0, (F.col(\"All_FU\") / F.col(\"All_First\"))\n",
    "    ), [\"All_FU\", \"All_First\"]),\n",
    "    (\"All_FU_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_FU\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_FU\", \"All_Total\"]),\n",
    "    (\"All_First_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_First\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_First\", \"All_Total\"]),\n",
    "    (\"All_NoProc_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_NoProc\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_NoProc\", \"All_Total\"]),\n",
    "    (\"All_Proc_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_Proc\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_Proc\", \"All_Total\"]),\n",
    "    (\"Remote_Total_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"Remote_Total\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"Remote_Total\", \"All_Total\"]),\n",
    "    (\"Remote_FU_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"Remote_FU\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"Remote_FU\", \"All_FU\"]),\n",
    "    (\"Remote_First_Over_All_First\", lambda d: F.when(\n",
    "        F.col(\"All_First\") != 0, (F.col(\"Remote_First\") / F.col(\"All_First\")) * 100\n",
    "    ), [\"Remote_First\", \"All_First\"]),\n",
    "    (\"F2F_DNA_Over_F2F_Total\", lambda d: F.when(\n",
    "        (F.col(\"F2F_Total\") + F.col(\"F2F_DNA\")) != 0,\n",
    "        (F.col(\"F2F_DNA\") / (F.col(\"F2F_Total\") + F.col(\"F2F_DNA\"))) * 100\n",
    "    ), [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_DNA_Over_Remote_Total\", lambda d: F.when(\n",
    "        (F.col(\"Remote_Total\") + F.col(\"Remote_DNA\")) != 0,\n",
    "        (F.col(\"Remote_DNA\") / (F.col(\"Remote_Total\") + F.col(\"Remote_DNA\"))) * 100\n",
    "    ), [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "\n",
    "for name, expr, req in metrics:\n",
    "    df = safe_add(df, name, expr, req)\n",
    "\n",
    "simple_copies = [\n",
    "    (\"All_DNA1\", \"All_DNA\"),\n",
    "    (\"All_DNA2\", \"All_DNA\"),\n",
    "    (\"All_First1\", \"All_First\"),\n",
    "    (\"All_First2\", \"All_First\"),\n",
    "    (\"All_First3\", \"All_First\"),\n",
    "    (\"All_FU1\", \"All_FU\"),\n",
    "    (\"All_FU2\", \"All_FU\"),\n",
    "    (\"All_FU3\", \"All_FU\"),\n",
    "    (\"All_FU4\", \"All_FU\"),\n",
    "    (\"All_FU5\", \"All_FU\"),\n",
    "    (\"All_Total1\", \"All_Total\"),\n",
    "    (\"All_Total2\", \"All_Total\"),\n",
    "    (\"All_Total3\", \"All_Total\"),\n",
    "    (\"All_Total4\", \"All_Total\"),\n",
    "    (\"All_Total5\", \"All_Total\"),\n",
    "    (\"All_Total6\", \"All_Total\"),\n",
    "    (\"Remote_Total1\", \"Remote_Total\"),\n",
    "    (\"Remote_Total2\", \"Remote_Total\"),\n",
    "]\n",
    "for newc, base in simple_copies:\n",
    "    if base in df.columns:\n",
    "        df = df.withColumn(newc, F.col(base))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "combos = [\n",
    "    (\"All_First_plus_All_First_DNA\", [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_plus_All_FU_DNA\", [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_Total_plus_All_DNA\", [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"F2F_Total_plus_F2F_DNA\", [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_Total_plus_Remote_DNA\", [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "for newc, cols in combos:\n",
    "    if all(c in df.columns for c in cols):\n",
    "        df = df.withColumn(newc, F.col(cols[0]).cast(\"long\") + F.col(cols[1]).cast(\"long\"))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "cols_to_drop = [c for c in [\"Der_Activity_Month\", \"Der_Provider_Code\"] if c in df.columns]\n",
    "opa_final_with_added_metrics = df.drop(*cols_to_drop)\n",
    "\n",
    "display(opa_final_with_added_metrics.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02eef9c-fc4e-4ee6-8f11-5f4cb9c538b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11 reshapes the wide outpatient dataset into a long (tidy) format for easier analysis\n",
    "from pyspark.sql.functions import col, explode, array, struct, lit, concat_ws\n",
    "\n",
    "# ID columns to keep\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    # \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\",\n",
    "    \"Adj Org Code\",\n",
    "    \"ICB\",\n",
    "    \"Region_Code\"\n",
    "]\n",
    "\n",
    "# Identify all metric columns\n",
    "metric_cols = [c for c in opa_final_with_added_metrics.columns if c not in id_cols]\n",
    "\n",
    "# Unpivot numeric metrics\n",
    "opa_long = (\n",
    "    opa_final_with_added_metrics\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"Metric_Name\"), col(c).alias(\"Metric_Value\")) for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the combined metric name\n",
    "opa_long = opa_long.withColumn(\n",
    "    \"Metric_Name_Treatment_Function_Group\",\n",
    "    concat_ws(\"_\", col(\"Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# Order by date\n",
    "opa_long_ordered = opa_long.orderBy(\"Der_Activity_Month_Date\")\n",
    "\n",
    "display(opa_long_ordered.limit(10))\n",
    "print(f\"Number of rows in opa_long_ordered: {opa_long_ordered.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdb2af-8682-41af-a49e-78981b93d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#12 – Aggregation and final metric derivation (Org, ICB, Region)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# Start from Org-level counts from Container 10\n",
    "df_org = opa_final_with_added_metrics.withColumnRenamed(\"Adj Org Code\", \"Adj_Org_Code\")\n",
    "\n",
    "# Base metric columns to sum (added F2F_DNA which was missing)\n",
    "count_cols = [\n",
    "    \"All_Total\",\"All_First\",\"All_FU\",\"All_Proc\",\"All_NoProc\",\n",
    "    \"All_FU_Proc\",\"All_FU_NoProc\",\n",
    "    \"F2F_Total\",\"F2F_First\",\"F2F_FU\",\"F2F_DNA\",\n",
    "    \"Remote_Total\",\"Remote_First\",\"Remote_FU\",\"Remote_DNA\",\n",
    "    \"All_DNA\",\"All_First_DNA\",\"All_FU_DNA\",\n",
    "    \"All_2WW\",\"All_First_2WW\",\"All_FU_2WW\",\n",
    "    \"All_2WW_DNA\",\"All_First_2WW_DNA\",\"All_FU_2WW_DNA\"\n",
    "]\n",
    "\n",
    "# Function to (re)calculate rates & derived metrics (now includes F2F_DNA_Over_F2F_Total)\n",
    "def add_rate_metrics(df):\n",
    "    return (\n",
    "        df\n",
    "        # DNA metrics\n",
    "        .withColumn(\"All_DNA_Over_All_Total\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "            (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_DNA_Over_All_Total_IG\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "            (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_DNA_Over_All_First\", F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "            (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_DNA_Over_All_First_IG\", F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "            (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_DNA_Over_All_FU\", F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "            (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_DNA_Over_All_FU_IG\", F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "            (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100).otherwise(None))\n",
    "        # FU metrics\n",
    "        .withColumn(\"All_FU_NoProc_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"All_FU_NoProc\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_Proc_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"All_FU_Proc\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_To_All_First\", F.when(F.col(\"All_First\")!=0,\n",
    "            (F.col(\"All_FU\")/F.col(\"All_First\"))).otherwise(None))\n",
    "        # 2WW rates\n",
    "        .withColumn(\"All_2WW_DNA_Over_All_2WW\", F.when(F.col(\"All_2WW\")!=0,\n",
    "            (F.col(\"All_2WW_DNA\")/F.col(\"All_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_2WW_DNA_Over_All_First_2WW\", F.when(F.col(\"All_First_2WW\")!=0,\n",
    "            (F.col(\"All_First_2WW_DNA\")/F.col(\"All_First_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_2WW_DNA_Over_All_FU_2WW\", F.when(F.col(\"All_FU_2WW\")!=0,\n",
    "            (F.col(\"All_FU_2WW_DNA\")/F.col(\"All_FU_2WW\"))*100).otherwise(None))\n",
    "        # Mix shares\n",
    "        .withColumn(\"All_FU_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_FU\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_First\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_NoProc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_NoProc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_Proc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_Proc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_Total_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"Remote_Total\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_FU_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"Remote_FU\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_First_Over_All_First\", F.when(F.col(\"All_First\")!=0,\n",
    "            (F.col(\"Remote_First\")/F.col(\"All_First\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_DNA_Over_Remote_Total\", F.when((F.col(\"Remote_Total\")+F.col(\"Remote_DNA\"))!=0,\n",
    "            (F.col(\"Remote_DNA\")/(F.col(\"Remote_Total\")+F.col(\"Remote_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"F2F_DNA_Over_F2F_Total\", F.when((F.col(\"F2F_Total\")+F.col(\"F2F_DNA\"))!=0,\n",
    "            (F.col(\"F2F_DNA\")/(F.col(\"F2F_Total\")+F.col(\"F2F_DNA\")))*100).otherwise(None))\n",
    "    )\n",
    "\n",
    "# Aggregate to ICB\n",
    "df_icb = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"ICB\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_icb = add_rate_metrics(df_icb).withColumn(\"Level\", F.lit(\"ICB\"))\n",
    "\n",
    "# Aggregate to Region\n",
    "df_region = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_region = add_rate_metrics(df_region).withColumn(\"Level\", F.lit(\"Region\"))\n",
    "\n",
    "# Label Org-level rows\n",
    "df_org = df_org.withColumn(\"Level\", F.lit(\"Org\"))\n",
    "\n",
    "# Combine all levels into one dataset\n",
    "final_output = (\n",
    "    df_org.unionByName(df_icb, allowMissingColumns=True)\n",
    "          .unionByName(df_region, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "# Adjust codes based on Level\n",
    "final_output = final_output.withColumn(\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    when(col(\"Level\") == \"Org\", col(\"Adj_Org_Code\"))\n",
    "    .when(col(\"Level\") == \"ICB\", col(\"ICB\"))\n",
    "    .when(col(\"Level\") == \"Region\", col(\"Region_Code\"))\n",
    ")\n",
    "\n",
    "# ---- NEW: re-create \"simple copy\" and \"combo\" columns at ICB/Region so they’re populated, not NULL ----\n",
    "copy_map = {\n",
    "    \"All_DNA1\":\"All_DNA\",\"All_DNA2\":\"All_DNA\",\n",
    "    \"All_FU1\":\"All_FU\",\"All_FU2\":\"All_FU\",\"All_FU3\":\"All_FU\",\"All_FU4\":\"All_FU\",\"All_FU5\":\"All_FU\",\n",
    "    \"All_Total1\":\"All_Total\",\"All_Total2\":\"All_Total\",\"All_Total3\":\"All_Total\",\n",
    "    \"All_Total4\":\"All_Total\",\"All_Total5\":\"All_Total\",\"All_Total6\":\"All_Total\",\n",
    "    \"Remote_Total1\":\"Remote_Total\",\"Remote_Total2\":\"Remote_Total\"\n",
    "}\n",
    "for newc, base in copy_map.items():\n",
    "    if base in final_output.columns:\n",
    "        final_output = final_output.withColumn(newc, F.col(base))\n",
    "\n",
    "combo_pairs = {\n",
    "    \"All_First_plus_All_First_DNA\": (\"All_First\",\"All_First_DNA\"),\n",
    "    \"All_FU_plus_All_FU_DNA\": (\"All_FU\",\"All_FU_DNA\"),\n",
    "    \"All_Total_plus_All_DNA\": (\"All_Total\",\"All_DNA\"),\n",
    "    \"F2F_Total_plus_F2F_DNA\": (\"F2F_Total\",\"F2F_DNA\"),\n",
    "    \"Remote_Total_plus_Remote_DNA\": (\"Remote_Total\",\"Remote_DNA\")\n",
    "}\n",
    "for newc, (a,b) in combo_pairs.items():\n",
    "    if a in final_output.columns and b in final_output.columns:\n",
    "        final_output = final_output.withColumn(newc, F.col(a).cast(\"long\") + F.col(b).cast(\"long\"))\n",
    "\n",
    "# Save (same as your original)\n",
    "final_output.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "# display(final_output.limit(10))\n",
    "# print(f\"Rows in final output: {final_output.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8aa02e-dd35-4b70-a01f-17d8baa7486c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#13 – long/skinny OPRT format (Level preserved)\n",
    "from pyspark.sql.functions import col, lit, explode, array, struct\n",
    "\n",
    "# Use the final_output table from container 12 (has Level + Adj_Org_Code_Final)\n",
    "df_wide = final_output\n",
    "\n",
    "# --- Step 1: Drop unnecessary columns (optional; keep if you don't need the raw counts downstream) ---\n",
    "cols_to_drop = [\n",
    "    \"Adj_Org_Code\",\n",
    "    \"All_DNA\", \"All_First\", \"All_FU\",\n",
    "    \"All_Total\", \"F2F_Total\", \"Remote_Total\"\n",
    "]\n",
    "df_wide = df_wide.drop(*[c for c in cols_to_drop if c in df_wide.columns])\n",
    "\n",
    "# --- Step 2: Define identifier columns (KEEP Level) ---\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Region_Code\",\n",
    "    \"ICB\",\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    \"Treatment_Function_Group\",\n",
    "    \"Level\",  # <-- critical fix\n",
    "]\n",
    "\n",
    "# --- Step 3: Identify metric columns (exclude identifiers) ---\n",
    "metric_cols = [c for c in df_wide.columns if c not in id_cols]\n",
    "\n",
    "# --- Step 4: Melt into long/skinny format ---\n",
    "opa_oprt_long = (\n",
    "    df_wide.select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"OPRT_Metric_Name\"), col(c).alias(\"Metric_Value\"))\n",
    "            for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.OPRT_Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Step 5: Optional renaming (kept for consistency) ---\n",
    "opa_oprt_long = opa_oprt_long.withColumnRenamed(\"Treatment_Function_Group\", \"Treatment_Function_Group\")\n",
    "\n",
    "display(opa_oprt_long.limit(10))\n",
    "print(f\"Container 13 complete — {opa_oprt_long.count()} rows, {len(opa_oprt_long.columns)} columns\")\n",
    "\n",
    "# Ensure Container 14 uses the cleaned long-format table\n",
    "final_output = opa_oprt_long\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17e7bc2-c4e7-41b4-a017-b261ac96fc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#14 — Join Internal IDs and Clean Long OPRT Dataset (no restacking; Level preserved)\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, concat_ws, lower, regexp_replace, trim\n",
    ")\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Step 1: Start from container 13 output (already has Level + Adj_Org_Code_Final) ---\n",
    "df_long = final_output\n",
    "\n",
    "# --- Step 2: DO NOT restack; just keep the dataset as-is ---\n",
    "df_stacked = df_long\n",
    "\n",
    "# --- Step 3: Filter out unwanted Treatment_Function_Group = 'Other' ---\n",
    "df_stacked = df_stacked.filter(col(\"Treatment_Function_Group\") != \"Other\")\n",
    "\n",
    "# --- Step 4: Build combined metric name for joining to ID list ---\n",
    "df_stacked = df_stacked.withColumn(\n",
    "    \"OPRT_Metric_Name_TFC\",\n",
    "    concat_ws(\"_\", col(\"OPRT_Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# --- Step 5: Normalize join keys on our long dataset ---\n",
    "# spaces -> underscores, drop non [a-z0-9_], collapse underscores, trim underscores, lowercase\n",
    "df_stacked_clean = df_stacked.withColumn(\n",
    "    \"join_metric\",\n",
    "    lower(regexp_replace(trim(col(\"OPRT_Metric_Name_TFC\")), r\"\\s+\", \"_\"))\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"[^a-z0-9_]\", \"\")\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"_+\", \"_\")\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"^_+|_+$\", \"\")\n",
    ")\n",
    "\n",
    "# --- Step 5b: Prepare mhs_metric_list cleaned keys (bring in Description too) ---\n",
    "metric_list_col = [c for c in mhs_metric_list.columns if \"OPRT\" in c and \"TFC\" in c]\n",
    "if len(metric_list_col) == 0:\n",
    "    raise ValueError(\"Could not find OPRT TFC metric column name in mhs_metric_list. Review mhs_metric_list columns.\")\n",
    "metric_list_col = metric_list_col[0]\n",
    "\n",
    "mhs_metric_list_clean = mhs_metric_list.withColumn(\n",
    "    \"join_metric\",\n",
    "    lower(regexp_replace(trim(col(metric_list_col)), r\"\\s+\", \"_\"))\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"[^a-z0-9_]\", \"\")\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"_+\", \"_\")\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"^_+|_+$\", \"\")\n",
    ")\n",
    "\n",
    "select_cols = [\"join_metric\", \"InternalID\"]\n",
    "if \"Description\" in mhs_metric_list_clean.columns:\n",
    "    select_cols.append(\"Description\")\n",
    "else:\n",
    "    print(\"WARNING: mhs_metric_list does not contain a column named 'Description'.\")\n",
    "\n",
    "mhs_metric_list_clean_sel = mhs_metric_list_clean.select(*select_cols).distinct()\n",
    "\n",
    "# --- Step 6a: Left join first — to capture unmatched metrics for debugging ---\n",
    "df_with_id_left = df_stacked_clean.join(\n",
    "    mhs_metric_list_clean_sel,\n",
    "    on=\"join_metric\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# show how many unmatched keys we have\n",
    "unmatched = df_with_id_left.filter(col(\"InternalID\").isNull()).select(\"OPRT_Metric_Name_TFC\").distinct()\n",
    "# print(\"Number of distinct unmatched OPRT metric keys:\", unmatched.count())\n",
    "\n",
    "# write unmatched to disk for inspection\n",
    "unmatched.write.mode(\"overwrite\").parquet(\"/mnt/output/unmatched_oprt_metrics.parquet\")\n",
    "# display(unmatched.limit(50))\n",
    "\n",
    "# Optional: inspect where '2WW' metrics went\n",
    "df_with_id_left.filter(lower(col(\"OPRT_Metric_Name\")).like(\"%2ww%\")) \\\n",
    "    .select(\"OPRT_Metric_Name_TFC\",\"InternalID\").distinct().show(200, truncate=False)\n",
    "\n",
    "# --- Step 6b: Final join for production (inner; drop unmapped) ---\n",
    "df_with_id = df_stacked_clean.join(\n",
    "    mhs_metric_list_clean_sel,\n",
    "    on=\"join_metric\",\n",
    "    how=\"inner\"\n",
    ").drop(\"join_metric\")\n",
    "\n",
    "# --- Step 7: Filter allowable org codes if provided ---\n",
    "if \"Org_Code\" in mhs_allowable_orgs.columns:\n",
    "    df_with_id = df_with_id.join(\n",
    "        mhs_allowable_orgs.select(col(\"Org_Code\").alias(\"allowable_code\")),\n",
    "        df_with_id[\"Adj_Org_Code_Final\"] == col(\"allowable_code\"),\n",
    "        \"inner\"\n",
    "    ).drop(\"allowable_code\")\n",
    "\n",
    "# --- Step 8: Enforce clean data types and ensure Description present ---\n",
    "df_with_id = (\n",
    "    df_with_id\n",
    "    .withColumn(\"Der_Activity_Month_Date\", col(\"Der_Activity_Month_Date\").cast(DateType()))\n",
    "    .withColumn(\"Adj_Org_Code_Final\", col(\"Adj_Org_Code_Final\").cast(StringType()))\n",
    "    .withColumn(\"Level\", col(\"Level\").cast(StringType()))\n",
    "    .withColumn(\"Treatment_Function_Group\", col(\"Treatment_Function_Group\").cast(StringType()))\n",
    "    .withColumn(\"OPRT_Metric_Name\", col(\"OPRT_Metric_Name\").cast(StringType()))\n",
    "    .withColumn(\"OPRT_Metric_Name_TFC\", col(\"OPRT_Metric_Name_TFC\").cast(StringType()))\n",
    "    .withColumn(\"Metric_Value\", col(\"Metric_Value\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "if \"Description\" not in df_with_id.columns:\n",
    "    df_with_id = df_with_id.withColumn(\"Description\", lit(None).cast(StringType()))\n",
    "\n",
    "# --- Step 9: Write final tidy dataset ---\n",
    "df_with_id.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_oprt_final\")\n",
    "\n",
    "# display(df_with_id.limit(20))\n",
    "# print(f\" Container 14 complete — {df_with_id.count()} rows, {len(df_with_id.columns)} columns\")\n",
    "# print(\"Unmatched metric keys written to /mnt/output/unmatched_oprt_metrics.parquet for inspection.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc65f15-e5a2-45db-9a01-2685810ed977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15 – Add Remote Lower Benchmark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start from container 10 output\n",
    "df_benchmark = opa_final_with_added_metrics\n",
    "\n",
    "# Step 1: Calculate 25th percentile of Remote_Total by month and Adj Org Code\n",
    "remote_lower = (\n",
    "    df_benchmark\n",
    "    .groupBy(\"Der_Activity_Month_Date\", \"Adj Org Code\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(Remote_Total, 0.25)\").alias(\"Remote_Lower_Benchmark\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Join benchmark back to main dataset\n",
    "df_with_benchmark = df_benchmark.join(\n",
    "    remote_lower,\n",
    "    on=[\"Der_Activity_Month_Date\", \"Adj Org Code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: For missing values, fill with 0\n",
    "df_with_benchmark = df_with_benchmark.fillna({\"Remote_Lower_Benchmark\": 0})\n",
    "\n",
    "# Step 4: Final outputs\n",
    "opa_final_with_remote_benchmark = df_with_benchmark\n",
    "\n",
    "# Create a standalone lower benchmark table for downstream use\n",
    "df_lower_bm = (\n",
    "    remote_lower\n",
    "    .withColumnRenamed(\"Adj Org Code\", \"Adj_Org_Code_Final\")\n",
    "    .select(\"Der_Activity_Month_Date\", \"Adj_Org_Code_Final\", \"Remote_Lower_Benchmark\")\n",
    ")\n",
    "\n",
    "# Optional: Save for reference or later join\n",
    "df_lower_bm.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_lower_benchmark\")\n",
    "\n",
    "#display(opa_final_with_remote_benchmark.limit(10))\n",
    "#print(f\"Container 15 complete — {opa_final_with_remote_benchmark.count()} rows, {len(opa_final_with_remote_benchmark.columns)} columns\")\n",
    "#print(f\"Lower benchmark table created — {df_lower_bm.count()} rows, {len(df_lower_bm.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c00711f-0354-4c39-9bf7-7688e1ca355c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#16 DNA opportunities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Start from container 10 output (with All_DNA_Over_All_Total metric)\n",
    "df = opa_final_with_added_metrics\n",
    "\n",
    "# Add Level column if missing (Org level)\n",
    "if \"Level\" not in df.columns:\n",
    "    df = df.withColumn(\"Level\", F.lit(\"Org\"))\n",
    "\n",
    "# Ignore Treatment_Function_Group and focus on Org/Level\n",
    "df_filtered = df.select(\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Adj Org Code\",\n",
    "    \"Level\",\n",
    "    \"All_DNA\",\n",
    "    \"All_Total\",\n",
    "    \"All_DNA_Over_All_Total\"\n",
    ")\n",
    "\n",
    "# Define a 6-month rolling window per Org/Level\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Adj Org Code\", \"Level\")\n",
    "          .orderBy(F.col(\"Der_Activity_Month_Date\").cast(\"long\"))\n",
    "          .rowsBetween(-5, 0)  # last 6 months including current\n",
    ")\n",
    "\n",
    "# Calculate rolling averages\n",
    "avg_df = df_filtered.withColumn(\n",
    "    \"Avg_All_DNA\", F.avg(\"All_DNA\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"Avg_DNA_rate\", F.avg(\"All_DNA_Over_All_Total\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate national median and 25th percentile for each month\n",
    "national_stats = (\n",
    "    df_filtered.groupBy(\"Der_Activity_Month_Date\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(All_DNA_Over_All_Total, 0.5)\").alias(\"National_Median\"),\n",
    "        F.expr(\"percentile_approx(All_DNA_Over_All_Total, 0.25)\").alias(\"Percentile_25\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join stats back to avg_df\n",
    "avg_df = avg_df.join(\n",
    "    national_stats,\n",
    "    on=\"Der_Activity_Month_Date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Apply DNA_Opportunity_reduction rules\n",
    "avg_df = avg_df.withColumn(\n",
    "    \"DNA_Opportunity_reduction\",\n",
    "    F.when(F.col(\"Avg_DNA_rate\").isNull(), \"No reduction\")\n",
    "     .when(F.col(\"Avg_All_DNA\").isNull() | (F.col(\"Avg_All_DNA\") == 0), \"No reduction\")\n",
    "     .when(F.col(\"Avg_DNA_rate\") > F.col(\"National_Median\"), \"25% reduction\")\n",
    "     .when((F.col(\"Avg_DNA_rate\") <= F.col(\"National_Median\")) & (F.col(\"Avg_DNA_rate\") > F.col(\"Percentile_25\")), \"15% reduction\")\n",
    "     .when(F.col(\"Avg_DNA_rate\") <= F.col(\"Percentile_25\"), \"10% reduction\")\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "# Calculate DNA_Opportunity as an integer\n",
    "avg_df = avg_df.withColumn(\n",
    "    \"DNA_Opportunity\",\n",
    "    F.when(F.col(\"DNA_Opportunity_reduction\") == \"No reduction\", F.lit(0))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"25% reduction\", F.round(0.25 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"15% reduction\", F.round(0.15 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"10% reduction\", F.round(0.10 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "# Optional: keep only one row per Org/Level for latest month\n",
    "latest_month_window = Window.partitionBy(\"Adj Org Code\", \"Level\").orderBy(F.col(\"Der_Activity_Month_Date\").desc())\n",
    "dna_opp_df = avg_df.withColumn(\"row_num\", F.row_number().over(latest_month_window)).filter(F.col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "display(dna_opp_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792acef0-e51c-4c4a-9bd7-d1af883640c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#17 — Post-run QA (robust): ICB/Region completeness for copy/combos & F2F DNA metrics\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=== Container 17: Post-run QA (read-only, robust) ===\")\n",
    "\n",
    "# ---------- 0) Try to read wide output from C12 (authoritative for column-based checks) ----------\n",
    "df_wide = None\n",
    "df_long = None\n",
    "\n",
    "# Load wide (C12)\n",
    "try:\n",
    "    df_wide = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "    print(\"Loaded wide table from /mnt/output/opa_final_all_levels\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load wide table (/mnt/output/opa_final_all_levels):\", str(e))\n",
    "\n",
    "# Load long (C14) for secondary checks\n",
    "try:\n",
    "    df_long = spark.read.parquet(\"/mnt/output/opa_oprt_final\")\n",
    "    print(\"Loaded long table from /mnt/output/opa_oprt_final\")\n",
    "except Exception as e:\n",
    "    if 'df_with_id' in locals():\n",
    "        df_long = df_with_id\n",
    "        print(\"Using in-memory df_with_id for long checks\")\n",
    "    else:\n",
    "        print(\"No long table available for OPRT-based checks.\")\n",
    "\n",
    "# ---------- 1) If we have the wide table, run column-based checks there ----------\n",
    "copy_cols  = [\n",
    "    \"All_DNA1\",\"All_DNA2\",\n",
    "    \"All_FU1\",\"All_FU2\",\"All_FU3\",\"All_FU4\",\"All_FU5\",\n",
    "    \"All_Total1\",\"All_Total2\",\"All_Total3\",\"All_Total4\",\"All_Total5\",\"All_Total6\",\n",
    "    \"Remote_Total1\",\"Remote_Total2\"\n",
    "]\n",
    "combo_cols = [\n",
    "    \"All_First_plus_All_First_DNA\",\n",
    "    \"All_FU_plus_All_FU_DNA\",\n",
    "    \"All_Total_plus_All_DNA\",\n",
    "    \"F2F_Total_plus_F2F_DNA\",\n",
    "    \"Remote_Total_plus_Remote_DNA\"\n",
    "]\n",
    "extra_cols = [\"F2F_DNA\", \"F2F_DNA_Over_F2F_Total\"]\n",
    "\n",
    "def null_summary(df, label, cols):\n",
    "    total = df.count()\n",
    "    print(f\"\\n--- {label}: {total:,} rows ---\")\n",
    "    if total == 0:\n",
    "        print(\"No rows to check.\")\n",
    "        return\n",
    "    cols_present = [c for c in cols if c in df.columns]\n",
    "    cols_missing = sorted(set(cols) - set(cols_present))\n",
    "    if cols_missing:\n",
    "        print(\"WARNING: Missing expected columns:\", cols_missing)\n",
    "    if not cols_present:\n",
    "        print(\"No expected columns present — skipping null summary.\")\n",
    "        return\n",
    "    exprs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in cols_present]\n",
    "    res = df.agg(*exprs).collect()[0].asDict()\n",
    "    any_nulls = [(k, int(v), round(100*float(v)/total,2)) for k,v in res.items() if v and v > 0]\n",
    "    if not any_nulls:\n",
    "        print(\"All checked columns are fully populated (no NULLs). ✅\")\n",
    "    else:\n",
    "        print(\"Columns with NULLs (count, % of rows):\")\n",
    "        for k, v, pct in sorted(any_nulls, key=lambda x: (-x[1], x[0])):\n",
    "            print(f\"  {k:30s} {v:8d} ({pct:5.2f}%)\")\n",
    "\n",
    "def spotcheck_copies(df, label, n=100):\n",
    "    print(f\"\\n--- Spot-check copy columns @ {label} (random {n} rows) ---\")\n",
    "    mapping = {\n",
    "        \"All_DNA1\":\"All_DNA\",\"All_DNA2\":\"All_DNA\",\n",
    "        \"All_FU1\":\"All_FU\",\"All_FU2\":\"All_FU\",\"All_FU3\":\"All_FU\",\"All_FU4\":\"All_FU\",\"All_FU5\":\"All_FU\",\n",
    "        \"All_Total1\":\"All_Total\",\"All_Total2\":\"All_Total\",\"All_Total3\":\"All_Total\",\n",
    "        \"All_Total4\":\"All_Total\",\"All_Total5\":\"All_Total\",\"All_Total6\":\"All_Total\",\n",
    "        \"Remote_Total1\":\"Remote_Total\",\"Remote_Total2\":\"Remote_Total\"\n",
    "    }\n",
    "    pairs = [(k,v) for k,v in mapping.items() if k in df.columns and v in df.columns]\n",
    "    if not pairs:\n",
    "        print(\"No copy columns present — skipping.\")\n",
    "        return\n",
    "    sample = df.orderBy(F.rand()).limit(n)\n",
    "    exprs = [F.sum(F.when(F.col(k) != F.col(v), 1).otherwise(0)).alias(k+\"_neq\") for k,v in pairs]\n",
    "    out = sample.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All copy columns match their base values in the sample. ✅\")\n",
    "    else:\n",
    "        print(\"Mismatches found in sample:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "def spotcheck_combos(df, label, n=100):\n",
    "    print(f\"\\n--- Spot-check combo columns @ {label} (random {n} rows) ---\")\n",
    "    pairs = {\n",
    "        \"All_First_plus_All_First_DNA\": (\"All_First\",\"All_First_DNA\"),\n",
    "        \"All_FU_plus_All_FU_DNA\": (\"All_FU\",\"All_FU_DNA\"),\n",
    "        \"All_Total_plus_All_DNA\": (\"All_Total\",\"All_DNA\"),\n",
    "        \"F2F_Total_plus_F2F_DNA\": (\"F2F_Total\",\"F2F_DNA\"),\n",
    "        \"Remote_Total_plus_Remote_DNA\": (\"Remote_Total\",\"Remote_DNA\")\n",
    "    }\n",
    "    usable = [(newc,a,b) for newc,(a,b) in pairs.items() if all(c in df.columns for c in [newc,a,b])]\n",
    "    if not usable:\n",
    "        print(\"No combo columns present — skipping.\")\n",
    "        return\n",
    "    sample = df.orderBy(F.rand()).limit(n)\n",
    "    exprs = [F.sum(F.when(F.col(newc) != (F.col(a).cast(\"long\")+F.col(b).cast(\"long\")), 1).otherwise(0)).alias(newc+\"_neq\")\n",
    "             for (newc,a,b) in usable]\n",
    "    out = sample.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All combo columns equal the sum of their parts in the sample. ✅\")\n",
    "    else:\n",
    "        print(\"Mismatches found in sample:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "# Replace the old rate_cols(...) in Container 17 with this:\n",
    "def rate_cols(df):\n",
    "    # pick only columns that clearly represent rates (contain '_Over_') or are explicitly named rate fields\n",
    "    explicit = {\"F2F_DNA_Over_F2F_Total\", \"Remote_DNA_Over_Remote_Total\"}\n",
    "    return sorted([c for c in df.columns if (\"_Over_\" in c) or (c in explicit)])\n",
    "\n",
    "\n",
    "def rate_bounds(df, label):\n",
    "    rcols = rate_cols(df)\n",
    "    if not rcols:\n",
    "        print(f\"\\n--- {label}: No rate columns detected — skipping. ---\")\n",
    "        return\n",
    "    print(f\"\\n--- {label}: rate bounds (0–100) ---\")\n",
    "    exprs = [F.sum(F.when((F.col(c) < 0) | (F.col(c) > 100), 1).otherwise(0)).alias(c) for c in rcols]\n",
    "    out = df.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All rates within [0,100] (ignoring NULLs). ✅\")\n",
    "    else:\n",
    "        print(\"Out-of-range rate values detected:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "if df_wide is not None:\n",
    "    df_icb    = df_wide.filter(F.col(\"Level\") == \"ICB\")\n",
    "    df_region = df_wide.filter(F.col(\"Level\") == \"Region\")\n",
    "\n",
    "    # 1A) Null coverage\n",
    "    cols_to_check = [c for c in (copy_cols + combo_cols + extra_cols) if c in df_wide.columns]\n",
    "    null_summary(df_icb, \"ICB level (wide)\", cols_to_check)\n",
    "    null_summary(df_region, \"Region level (wide)\", cols_to_check)\n",
    "\n",
    "    # 1B) Spotchecks\n",
    "    spotcheck_copies(df_icb, \"ICB (wide)\")\n",
    "    spotcheck_copies(df_region, \"Region (wide)\")\n",
    "    spotcheck_combos(df_icb, \"ICB (wide)\")\n",
    "    spotcheck_combos(df_region, \"Region (wide)\")\n",
    "\n",
    "    # 1C) Rate bounds\n",
    "    rate_bounds(df_icb, \"ICB (wide)\")\n",
    "    rate_bounds(df_region, \"Region (wide)\")\n",
    "else:\n",
    "    print(\"\\nWide table unavailable — skipping column-based checks.\")\n",
    "\n",
    "# ---------- 2) If we have the long OPRT table, check presence of these metrics by name ----------\n",
    "if df_long is not None:\n",
    "    print(\"\\n--- OPRT long checks (by metric name) ---\")\n",
    "    target_metric_names = [  # these are expected as OPRT_Metric_Name entries if they exist upstream\n",
    "        \"All_DNA1\",\"All_DNA2\",\n",
    "        \"All_FU1\",\"All_FU2\",\"All_FU3\",\"All_FU4\",\"All_FU5\",\n",
    "        \"All_Total1\",\"All_Total2\",\"All_Total3\",\"All_Total4\",\"All_Total5\",\"All_Total6\",\n",
    "        \"Remote_Total1\",\"Remote_Total2\",\n",
    "        \"All_First_plus_All_First_DNA\",\n",
    "        \"All_FU_plus_All_FU_DNA\",\n",
    "        \"All_Total_plus_All_DNA\",\n",
    "        \"F2F_Total_plus_F2F_DNA\",\n",
    "        \"Remote_Total_plus_Remote_DNA\",\n",
    "        \"F2F_DNA\",\"F2F_DNA_Over_F2F_Total\"\n",
    "    ]\n",
    "    present = (df_long\n",
    "        .filter(F.col(\"Level\").isin(\"ICB\",\"Region\"))\n",
    "        .groupBy(\"OPRT_Metric_Name\")\n",
    "        .count()\n",
    "        .filter(F.col(\"OPRT_Metric_Name\").isin(target_metric_names))\n",
    "        .orderBy(\"OPRT_Metric_Name\"))\n",
    "\n",
    "    print(\"Presence of target metric names at ICB/Region in long table:\")\n",
    "    display(present)\n",
    "\n",
    "    # Null share by Level for those targets\n",
    "    mv_nulls = (df_long\n",
    "        .filter(F.col(\"Level\").isin(\"ICB\",\"Region\"))\n",
    "        .filter(F.col(\"OPRT_Metric_Name\").isin(target_metric_names))\n",
    "        .groupBy(\"Level\",\"OPRT_Metric_Name\")\n",
    "        .agg(F.count(\"*\").alias(\"rows\"),\n",
    "             F.sum(F.when(F.col(\"Metric_Value\").isNull(),1).otherwise(0)).alias(\"nulls\"),\n",
    "             F.round(F.sum(F.when(F.col(\"Metric_Value\").isNull(),1).otherwise(0))/F.count(\"*\")*100,2).alias(\"null_pct\"))\n",
    "        .orderBy(\"Level\",\"OPRT_Metric_Name\"))\n",
    "\n",
    "    print(\"Null share for those metrics (long):\")\n",
    "    display(mv_nulls)\n",
    "else:\n",
    "    print(\"\\nLong table unavailable — skipping OPRT checks.\")\n",
    "\n",
    "print(\"\\n=== QA complete. ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b26eaa7-0010-433f-b2f7-bead2585dd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#18 sanity check on nulls in aggregation\n",
    "from pyspark.sql import functions as F\n",
    "df = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "for lvl in [\"ICB\",\"Region\"]:\n",
    "    d = df.filter(F.col(\"Level\")==lvl)\n",
    "    n_null = d.filter(F.col(\"F2F_DNA_Over_F2F_Total\").isNull()).count()\n",
    "    n_zero = d.filter(((F.col(\"F2F_Total\").isNull()) | (F.col(\"F2F_Total\")==0)) &\n",
    "                      ((F.col(\"F2F_DNA\").isNull())  | (F.col(\"F2F_DNA\")==0))).count()\n",
    "    print(lvl, \"NULL rates:\", n_null, \"| zero denominators:\", n_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ec1ea8-fdbf-4a9b-9ae5-f3ccf26c90b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#19 — QA audit writer (Delta + ADLS CSV export)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== Container 18: QA audit writer (Delta + ADLS) ===\")\n",
    "\n",
    "# ---------- 0) Load inputs ----------\n",
    "df_wide = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "print(\"Loaded wide table from /mnt/output/opa_final_all_levels\")\n",
    "\n",
    "df_long = None\n",
    "try:\n",
    "    df_long = spark.read.parquet(\"/mnt/output/opa_oprt_final\")\n",
    "    print(\"Loaded long table from /mnt/output/opa_oprt_final\")\n",
    "except Exception as e:\n",
    "    print(\"No long table:\", str(e))\n",
    "\n",
    "unmatched = None\n",
    "try:\n",
    "    unmatched = spark.read.parquet(\"/mnt/output/unmatched_oprt_metrics.parquet\").distinct()\n",
    "    print(\"Loaded unmatched metrics.\")\n",
    "except Exception as e:\n",
    "    print(\"No unmatched metrics file:\", str(e))\n",
    "\n",
    "# ---------- 1) Helpers ----------\n",
    "run_ts = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def rate_cols(df):\n",
    "    explicit = {\"F2F_DNA_Over_F2F_Total\", \"Remote_DNA_Over_Remote_Total\"}\n",
    "    return sorted([c for c in df.columns if (\"_Over_\" in c) or (c in explicit)])\n",
    "\n",
    "rate_columns = rate_cols(df_wide)\n",
    "levels = [\"Org\", \"ICB\", \"Region\"]\n",
    "df_levels = {lvl: df_wide.filter(F.col(\"Level\") == lvl) for lvl in levels}\n",
    "\n",
    "# ---------- 2) Build summaries ----------\n",
    "summary_rows = []\n",
    "for lvl, d in df_levels.items():\n",
    "    total = d.count()\n",
    "    has_f2f = all(c in d.columns for c in [\"F2F_Total\",\"F2F_DNA\",\"F2F_DNA_Over_F2F_Total\"])\n",
    "    null_rate = d.filter(F.col(\"F2F_DNA_Over_F2F_Total\").isNull()).count() if has_f2f else None\n",
    "    denom_zero = d.filter(\n",
    "        (F.col(\"F2F_Total\").isNull() | (F.col(\"F2F_Total\")==0)) &\n",
    "        (F.col(\"F2F_DNA\").isNull()  | (F.col(\"F2F_DNA\")==0))\n",
    "    ).count() if has_f2f else None\n",
    "\n",
    "    oor_total = sum(\n",
    "        d.filter((F.col(rc) < 0) | (F.col(rc) > 100)).count()\n",
    "        for rc in rate_columns\n",
    "    )\n",
    "    summary_rows.append((run_ts, lvl, total, null_rate, denom_zero, oor_total))\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"run_ts\", T.StringType(), False),\n",
    "    T.StructField(\"Level\", T.StringType(), False),\n",
    "    T.StructField(\"rows\", T.LongType(), True),\n",
    "    T.StructField(\"F2F_DNA_rate_nulls\", T.LongType(), True),\n",
    "    T.StructField(\"F2F_DNA_zero_denoms\", T.LongType(), True),\n",
    "    T.StructField(\"rates_out_of_range_total\", T.LongType(), True),\n",
    "])\n",
    "qa_summary = spark.createDataFrame(summary_rows, schema)\n",
    "\n",
    "detail_rows = []\n",
    "for lvl, d in df_levels.items():\n",
    "    for rc in rate_columns:\n",
    "        cnt = d.filter((F.col(rc) < 0) | (F.col(rc) > 100)).count()\n",
    "        detail_rows.append((run_ts, lvl, rc, cnt))\n",
    "\n",
    "detail_schema = T.StructType([\n",
    "    T.StructField(\"run_ts\", T.StringType(), False),\n",
    "    T.StructField(\"Level\", T.StringType(), False),\n",
    "    T.StructField(\"rate_column\", T.StringType(), False),\n",
    "    T.StructField(\"out_of_range_count\", T.LongType(), False),\n",
    "])\n",
    "qa_rate_detail = spark.createDataFrame(detail_rows, detail_schema)\n",
    "\n",
    "target_metric_names = [\n",
    "    \"All_DNA1\",\"All_DNA2\",\"All_FU1\",\"All_FU2\",\"All_FU3\",\"All_FU4\",\"All_FU5\",\n",
    "    \"All_Total1\",\"All_Total2\",\"All_Total3\",\"All_Total4\",\"All_Total5\",\"All_Total6\",\n",
    "    \"Remote_Total1\",\"Remote_Total2\",\n",
    "    \"All_First_plus_All_First_DNA\",\"All_FU_plus_All_FU_DNA\",\"All_Total_plus_All_DNA\",\n",
    "    \"F2F_Total_plus_F2F_DNA\",\"Remote_Total_plus_Remote_DNA\",\n",
    "    \"F2F_DNA\",\"F2F_DNA_Over_F2F_Total\"\n",
    "]\n",
    "\n",
    "qa_long_presence = None\n",
    "if df_long is not None:\n",
    "    lp = (df_long.filter(F.col(\"Level\").isin(\"ICB\",\"Region\"))\n",
    "          .groupBy(\"Level\",\"OPRT_Metric_Name\")\n",
    "          .count()\n",
    "          .withColumn(\"run_ts\", F.lit(run_ts)))\n",
    "    qa_long_presence = lp.select(\"run_ts\",\"Level\",\"OPRT_Metric_Name\",\"count\")\n",
    "\n",
    "qa_unmatched = None\n",
    "if unmatched is not None:\n",
    "    qa_unmatched = (unmatched.groupBy()\n",
    "                    .agg(F.countDistinct(\"OPRT_Metric_Name_TFC\").alias(\"unmatched_metrics\"))\n",
    "                    .withColumn(\"run_ts\", F.lit(run_ts))\n",
    "                    .select(\"run_ts\",\"unmatched_metrics\"))\n",
    "\n",
    "# ---------- 3) Write as Delta (for history) ----------\n",
    "delta_base = \"/mnt/output/_qa_delta\"\n",
    "(qa_summary.write.format(\"delta\").mode(\"append\").save(f\"{delta_base}/summary_level\"))\n",
    "(qa_rate_detail.write.format(\"delta\").mode(\"append\").save(f\"{delta_base}/rate_detail\"))\n",
    "if qa_long_presence is not None:\n",
    "    (qa_long_presence.write.format(\"delta\").mode(\"append\").save(f\"{delta_base}/long_presence\"))\n",
    "if qa_unmatched is not None:\n",
    "    (qa_unmatched.write.format(\"delta\").mode(\"append\").save(f\"{delta_base}/unmatched_summary\"))\n",
    "\n",
    "print(\"Appended all QA datasets as Delta.\")\n",
    "\n",
    "# ---------- 4) Export CSV snapshot to ADLS ----------\n",
    "adls_csv_path = (\n",
    "    \"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/\"\n",
    "    \"ElectiveRecovery/Projects/OP_QA_STP_Region_all_metrics_Audit.csv\"\n",
    ")\n",
    "\n",
    "csv_summary = (\n",
    "    qa_summary\n",
    "    .withColumn(\"rates_out_of_range_any\", (F.col(\"rates_out_of_range_total\") > 0).cast(\"boolean\"))\n",
    ")\n",
    "\n",
    "(csv_summary.coalesce(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(adls_csv_path))\n",
    "\n",
    "print(f\"Wrote CSV snapshot to {adls_csv_path}\")\n",
    "print(\"=== QA audit writer complete ===\")\n",
    "\n",
    "display(qa_summary.orderBy(\"Level\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156c85e8-7836-4ddc-8b9b-c39c915e40df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#20 Saving the file to the lake mart for QA (filtered for a small sample)\n",
    "df_sample = df_with_id.filter(\n",
    "    (F.col(\"Der_Activity_Month_Date\") == \"2025-07-31\") & \n",
    "    (F.col(\"Adj_Org_Code_Final\") == \"RH8\")\n",
    "    #(F.col(\"ICB\") == \"QE1\")\n",
    "    #(F.col(\"Region_Code\") == \"Y59\")\n",
    ")\n",
    "df_sample_count = df_sample.count()\n",
    "#print(f\"Number of rows in filtered sample: {df_sample_count}\")\n",
    "\n",
    "df_sample.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_short_Y59.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1125d19-3066-4171-b99c-95955e2286a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#21 Saving the file to the lake mart for QA\n",
    "df_with_id.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_new.csv\")\n",
    "\n",
    "#display(df_with_id.limit(10))\n",
    "#print(f\"Number of rows in df_with_id: {df_with_id.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1456ecc5-0235-47d9-a62e-39906eb80b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#22 Saving the file to the lake mart for QA (filtered for a small sample)\n",
    "opa_final_with_remote_benchmark.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/lower_bm_new.csv\")\n",
    "\n",
    "#display(opa_final_with_remote_benchmark.limit(10))\n",
    "#print(f\"Number of rows in opa_final_with_remote_benchmark: {opa_final_with_remote_benchmark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e918b0f-51ad-49d8-9595-f9954b2c5d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#23 Saving the file to the lake mart for QA (filtered for a small sample)\n",
    "dna_opp_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/DNA_Opportunities_New.csv\")\n",
    "\n",
    "#display(dna_opp_df.limit(10))\n",
    "#print(f\"Number of rows in dna_opp_df: {dna_opp_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42149a59-cafe-4c71-8261-340eff8521a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step Container 9a\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "id_cols = [\"Der_Activity_Month_Date\", \"ICB\", \"Region_Code\", \"Adj Org Code\"]\n",
    "metric_cols = [c for c in opa_final_processed.columns if c not in id_cols + [\"Treatment_Function_Group\"]]\n",
    "\n",
    "sum_by_groups = (\n",
    "  opa_final_processed\n",
    "  .filter(F.col(\"Treatment_Function_Group\") != \"All\")\n",
    "  .groupBy(*id_cols)\n",
    "  .agg(*[F.sum(F.col(c)).alias(c + \"_sum_groups\") for c in metric_cols])\n",
    ")\n",
    "\n",
    "all_rows = (\n",
    "  opa_final_processed\n",
    "  .filter(F.col(\"Treatment_Function_Group\") == \"All\")\n",
    "  .select(*id_cols, *metric_cols)\n",
    "  .toDF(*id_cols, *[c + \"_all\" for c in metric_cols])\n",
    ")\n",
    "\n",
    "cmp = sum_by_groups.join(all_rows, on=id_cols, how=\"inner\")\n",
    "\n",
    "mismatches = []\n",
    "for c in metric_cols:\n",
    "    mismatches.append(F.sum(F.when(F.col(c + \"_sum_groups\") != F.col(c + \"_all\"), 1).otherwise(0)).alias(c+\"_neq\"))\n",
    "\n",
    "res = cmp.agg(*mismatches).collect()[0].asDict()\n",
    "[ (k,v) for k,v in res.items() if v != 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ff76b4-5f91-4da6-953b-71225c90725e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step Container 9b\n",
    "bad = cmp.filter((F.col(\"All_Total_sum_groups\") != F.col(\"All_Total_all\"))).limit(50)\n",
    "display(bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b814c761-6c01-4c34-9c13-8769eb87d512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step Container 9c\n",
    "distinct_groups = opa_final_processed.select(\"Treatment_Function_Group\").distinct().collect()\n",
    "distinct_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f10ea43b-01ea-4f65-915f-53cf0759ea13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step Container 10a\n",
    "pct_cols = [c for c in opa_final_with_added_metrics.columns if c.endswith(\"_Over_All_Total\") or c.endswith(\"_Over_All_FU\") or c.endswith(\"_Over_All_First\") or c.endswith(\"_2WW\") or c in [\"F2F_DNA_Over_F2F_Total\",\"Remote_DNA_Over_Remote_Total\"]]\n",
    "\n",
    "violations = (\n",
    "  opa_final_with_added_metrics\n",
    "  .select(*[F.sum(F.when((F.col(c) < 0) | (F.col(c) > 100), 1).otherwise(0)).alias(c) for c in pct_cols])\n",
    "  .collect()[0].asDict()\n",
    ")\n",
    "[ (k,v) for k,v in violations.items() if v != 0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "611e5a99-2f53-4865-afb9-2903992f7126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step Container 10b\n",
    "checks = opa_final_with_added_metrics.select(\n",
    "  F.sum(F.when(F.col(\"All_Total_plus_All_DNA\") != (F.col(\"All_Total\").cast(\"long\")+F.col(\"All_DNA\").cast(\"long\")), 1).otherwise(0)).alias(\"all_att_plus_dna_mismatch\"),\n",
    "  F.sum(F.when(F.col(\"F2F_Total_plus_F2F_DNA\") != (F.col(\"F2F_Total\").cast(\"long\")+F.col(\"F2F_DNA\").cast(\"long\")), 1).otherwise(0)).alias(\"f2f_att_plus_dna_mismatch\"),\n",
    "  F.sum(F.when(F.col(\"Remote_Total_plus_Remote_DNA\") != (F.col(\"Remote_Total\").cast(\"long\")+F.col(\"Remote_DNA\").cast(\"long\")), 1).otherwise(0)).alias(\"remote_att_plus_dna_mismatch\")\n",
    ").collect()[0]\n",
    "checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dfc91c2-45fd-4e94-b6d2-97bb58620e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 11\n",
    "id_cols = [\"Der_Activity_Month_Date\",\"Treatment_Function_Group\",\"Adj Org Code\",\"ICB\",\"Region_Code\"]\n",
    "m_cols = [c for c in opa_final_with_added_metrics.columns if c not in id_cols]\n",
    "\n",
    "wide_cells = opa_final_with_added_metrics.count() * len(m_cols)\n",
    "long_rows  = opa_long_ordered.count()\n",
    "(wide_cells, long_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2323419e-92b6-427e-8aa1-c60f40fdd42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 12\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample ICB parity check\n",
    "sample_icb = (\n",
    "  final_output\n",
    "  .filter(F.col(\"Level\")==\"ICB\")\n",
    "  .select(\"Der_Activity_Month_Date\",\"ICB\",\"Treatment_Function_Group\",\"All_DNA\",\"All_Total\",\"All_DNA_Over_All_Total\")\n",
    "  .withColumn(\"recalc\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0, (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100))\n",
    "  .withColumn(\"neq\", F.when((F.abs(F.col(\"recalc\")-F.col(\"All_DNA_Over_All_Total\")) > 1e-9) | (F.col(\"recalc\").isNull() ^ F.col(\"All_DNA_Over_All_Total\").isNull()), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "sample_icb.agg(F.sum(\"neq\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f31079-b68f-43c3-9fdb-cbb50a6b1d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 13\n",
    "id_cols = [\"Der_Activity_Month_Date\",\"Region_Code\",\"ICB\",\"Adj_Org_Code_Final\",\"Treatment_Function_Group\",\"Level\"]\n",
    "metric_cols = [c for c in final_output.columns if c not in id_cols]  # final_output is wide before melt in C13\n",
    "\n",
    "# If final_output has already been overwritten by long, reload the C12 parquet:\n",
    "# df_wide_c12 = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")  # assuming you switch to Delta/Parquet path used in C12\n",
    "# metric_cols = [c for c in df_wide_c12.columns if c not in id_cols]\n",
    "\n",
    "# Compare cell counts (if you have both wide and long at hand)\n",
    "# wide_cells = df_wide_c12.count() * len(metric_cols)\n",
    "# long_rows  = opa_oprt_long.count()\n",
    "# (wide_cells, long_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856da59b-d92b-4ef1-a555-83675d9eeaed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 14a\n",
    "# Left-joined version produced earlier:\n",
    "df_left = spark.read.parquet(\"/mnt/output/unmatched_oprt_metrics.parquet\")  # written by C14\n",
    "unmatched_cnt = df_left.distinct().count()\n",
    "unmatched_cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37598a99-9973-40ef-85c3-a1b1ad43f203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 14b\n",
    "from pyspark.sql.types import DoubleType, DateType, StringType\n",
    "df_with_id.printSchema()  # confirm Date/Double/String as intended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d20f45f-976f-4284-9ab1-37c4032861f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 15\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "probe = (\n",
    "  opa_final_with_remote_benchmark\n",
    "  .select(\"Der_Activity_Month_Date\",\"Adj Org Code\",\"Remote_Total\",\"Remote_Lower_Benchmark\")\n",
    "  .withColumn(\"same\", F.when(F.col(\"Remote_Total\") == F.col(\"Remote_Lower_Benchmark\"), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "probe.agg(F.sum(\"same\").alias(\"same_cnt\"), F.count(\"*\").alias(\"rows\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc98643-aa2b-49fd-af39-360a7d68528e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 16a\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dna_checks = dna_opp_df.select(\n",
    "  F.sum(F.when(F.col(\"DNA_Opportunity\") < 0, 1).otherwise(0)).alias(\"negatives\"),\n",
    "  F.sum(F.when(F.col(\"DNA_Opportunity\").isNotNull() & (F.col(\"DNA_Opportunity\") != F.col(\"DNA_Opportunity\").cast(\"int\")), 1).otherwise(0)).alias(\"non_ints\")\n",
    ").collect()[0].asDict()\n",
    "dna_checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4b4b4e-7fed-46c4-bc2b-6e8979205329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step container 16b\n",
    "dna_opp_df.groupBy(\"DNA_Opportunity_reduction\").count().orderBy(\"DNA_Opportunity_reduction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa9804e-9d98-4422-8d59-e166ba6e5b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step final wide table\n",
    "id_cols = [\"Der_Activity_Month_Date\",\"Treatment_Function_Group\",\"Region_Code\",\"ICB\",\"Adj Org Code\"]\n",
    "dupes = (\n",
    "  opa_final_processed.groupBy(*id_cols)\n",
    "  .count()\n",
    "  .filter(\"count > 1\")\n",
    "  .count()\n",
    ")\n",
    "dupes  # expect 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3efffa7c-fb85-427f-9a55-4ce28308b88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step final long table\n",
    "id_cols = [\"Der_Activity_Month_Date\",\"Adj_Org_Code_Final\",\"Level\",\"Treatment_Function_Group\",\"InternalID\"]\n",
    "dupes = (\n",
    "  df_with_id.groupBy(*id_cols)\n",
    "  .count()\n",
    "  .filter(\"count > 1\")\n",
    "  .count()\n",
    ")\n",
    "dupes  # expect 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd523b5-f2bb-4000-80ba-bc554a497999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#QA step voulme reconciliation v source\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# After all filters in C9\n",
    "src = (\n",
    "  opa_with_groups\n",
    "  .filter(\n",
    "    (F.col(\"Der_Financial_Year\").isin(\"2023/24\",\"2024/25\",\"2025/26\")) &\n",
    "    (F.col(\"Administrative_Category\")==\"01\") &\n",
    "    (F.col(\"Treatment_Function_Code\")!=\"812\") &\n",
    "    (F.col(\"First_Attendance\").isin(\"1\",\"2\",\"3\",\"4\"))\n",
    "  )\n",
    ")\n",
    "\n",
    "att = src.filter(F.col(\"Attendance_Status\").isin(\"5\",\"6\")).count()\n",
    "dna = src.filter(F.col(\"Attendance_Status\").isin(\"3\",\"7\")).count()\n",
    "\n",
    "rollup = opa_final_with_added_metrics.select(\n",
    "  F.sum(\"All_Total\").alias(\"att_total\"),\n",
    "  F.sum(\"All_DNA\").alias(\"dna_total\")\n",
    ").first()\n",
    "\n",
    "(att, dna, rollup.att_total, rollup.dna_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0fc6258-164f-4fde-a566-f0be73655cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What the QA steps aims are\n",
    "\n",
    "Confidence that “All” equals the sum across TFC groups.\n",
    "\n",
    "All percentages are bounded, denominators match the intended definitions.\n",
    "\n",
    "Long/wide reshapes conserve information.\n",
    "\n",
    "Aggregated levels recompute rates correctly.\n",
    "\n",
    "Mapping to Internal IDs is complete (or you get a tidy list to fill).\n",
    "\n",
    "A demonstration (not a change) that the current remote benchmark equals the org’s own value, so you can sign off current behavior before changing it later.\n",
    "\n",
    "Opportunity numbers are non-negative integers with sensible band coverage.\n",
    "\n",
    "Uniqueness at expected grains and volume reconciliation with the filtered source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f098e-72cf-4778-982b-c3020bd3aba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Workspace/Repos/MHS-analytics/MHS-analytics/Python_Packages')\n",
    "# from mhs_import import MHS_IngestionHub\n",
    "# from mhs_db_config import INSERT, DELETE\n",
    "\n",
    "# def main():\n",
    "#     max_date = df_sample.agg(F.max(\"reportingDate\")).collect()[0][0]\n",
    "#     desc = f\"df_sample_{max_date}\"\n",
    "#     InjectionHub(df_sample, desc, True)\n",
    "#     print(\"df_sample sent to InjectionHub\")\n",
    "\n",
    "# def InjectionHub(dfih, desc, tf):\n",
    "#     sdf = dfih.withColumn('reportingDate', F.to_date('reportingDate'))\n",
    "#     display(sdf.sample(False, 0.01))\n",
    "#     display(sdf.dtypes)\n",
    "#     MHS_IngestionHub.upload(\n",
    "#         mhs_df=sdf,\n",
    "#         description=desc,\n",
    "#         loaded_by=\"steven.evans4@nhs.net\",\n",
    "#         mhs_mode=INSERT,\n",
    "#         skip_existing_data_check=tf\n",
    "#     )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f5a207-1000-4365-9ce9-e6e91057ca2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import Row\n",
    "\n",
    "#col_names = opa_final_with_added_metrics.columns\n",
    "#df_col_names = spark.createDataFrame([Row(Column_Name=c) for c in col_names])\n",
    "#display(df_col_names)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7 Re-shaping the data v4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
